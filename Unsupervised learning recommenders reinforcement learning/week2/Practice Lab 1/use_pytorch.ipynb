{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from recsys_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "X, W, b, num_movies, num_features, num_users = load_precalc_params_small()\n",
    "Y, R = load_ratings_small()\n",
    "\n",
    "print(\"Y\", Y.shape, \"R\", R.shape)\n",
    "print(\"X\", X.shape)\n",
    "print(\"W\", W.shape)\n",
    "print(\"b\", b.shape)\n",
    "print(\"num_features\", num_features)\n",
    "print(\"num_movies\",   num_movies)\n",
    "print(\"num_users\",    num_users)\n",
    "\n",
    "# print(X)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  From the matrix, we can compute statistics like average rating.\n",
    "tsmean =  np.mean(Y[0, R[0, :].astype(bool)])\n",
    "print(f\"Average rating for movie 1 : {tsmean:0.3f} / 5\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "X = torch.tensor(X, requires_grad=True)\n",
    "R = torch.tensor(R)\n",
    "Y = torch.tensor(Y)\n",
    "# 是不是复制别人的参数就不能作为grad\n",
    "# 但是可以手动设置requires_grad=True\n",
    "W = torch.tensor(W, requires_grad=True)\n",
    "b = torch.tensor(b, requires_grad=True)\n",
    "\n",
    "print(X.is_leaf, W.is_leaf, b.is_leaf, Y.is_leaf, R.is_leaf)\n",
    "print(X.requires_grad, W.requires_grad, b.requires_grad, Y.requires_grad, R.requires_grad)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# loss = torch.sum(torch.mul(R, torch.pow(torch.mm(F, W.t()) + b - Y, 2))) / 2\n",
    "\n",
    "def loss_func(X, W, b, Y, R, lambda_):\n",
    "    \"\"\"\n",
    "    Returns the cost for the content-based filtering\n",
    "    Args:\n",
    "      X (ndarray (num_movies,num_features)): matrix of item features\n",
    "      W (ndarray (num_users,num_features)) : matrix of user parameters\n",
    "      b (ndarray (1, num_users)            : vector of user parameters\n",
    "      Y (ndarray (num_movies,num_users)    : matrix of user ratings of movies\n",
    "      R (ndarray (num_movies,num_users)    : matrix, where R(i, j) = 1 if the i-th movies was rated by the j-th user\n",
    "      lambda_ (float): regularization parameter\n",
    "    Returns:\n",
    "      J (float) : Cost\n",
    "    \"\"\"\n",
    "    # define loss function\n",
    "    X = torch.Tensor(X)\n",
    "    W = torch.Tensor(W)\n",
    "    b = torch.Tensor(b)\n",
    "    R = torch.Tensor(R)\n",
    "    Y = torch.Tensor(Y)\n",
    "    # torch.mm torch.matmul都是matrix multiplication\n",
    "    # 但是torch.mm不会boradcast\n",
    "    J = torch.sum(torch.mul(R, torch.pow(torch.mm(X, W.t()) + b - Y, 2))) / 2\n",
    "    # considier regularization\n",
    "    J += lambda_ * torch.sum(torch.pow(W, 2)) / 2\n",
    "    J += lambda_ * torch.sum(torch.pow(X, 2)) / 2\n",
    "    return J.item()\n",
    "\n",
    "def loss_fun_tensor(X, W, b, Y, R, lambda_):\n",
    "    J = torch.sum(torch.mul(R, torch.pow(torch.mm(X, W.t()) + b - Y, 2))) / 2\n",
    "    # considier regularization\n",
    "    # J += lambda_ * torch.sum(torch.pow(W, 2)) / 2\n",
    "    # J += lambda_ * torch.sum(torch.pow(X, 2)) / 2\n",
    "    return J\n",
    "\n",
    "\n",
    "# Public tests\n",
    "from public_tests import *\n",
    "test_cofi_cost_func(loss_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do gradient descent\n",
    "# loss = loss_fun_tensor(X, W, b, Y, R, 0.1)\n",
    "# loss.backward()\n",
    "\n",
    "# J = torch.sum(torch.mul(R, torch.pow(torch.mm(X, W.t()) + b - Y, 2))) / 2\n",
    "lambda_ = 0.1\n",
    "J = 0\n",
    "J = torch.sum(torch.mul(R, torch.pow(torch.mm(X, W.t()) + b - Y, 2))) / 2\n",
    "# considier regularization\n",
    "J += lambda_ * torch.sum(torch.pow(W, 2)) / 2\n",
    "J += lambda_ * torch.sum(torch.pow(X, 2)) / 2\n",
    "\n",
    "\n",
    "# J = torch.sum(torch.mm(X, W.t()) + b - Y)\n",
    "print(J.item())\n",
    "J.backward()\n",
    "\n",
    "# X为什么不是leaf node\n",
    "print(b.requires_grad)\n",
    "print(b.is_leaf)\n",
    "print(b.grad)\n",
    "\n",
    "# 终于能算出数来了\n",
    "# 问题来了 怎么使用optimizer呢\n",
    "# 就可以直接这么用吗??\n",
    "# 也对啊 毕竟导数就存放在这些tensor里面\n",
    "optimizer = torch.optim.SGD([W, b, X], lr=0.01)\n",
    "optimizer.step()\n",
    "# 清空之前计算的导数\n",
    "optimizer.zero_grad()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新测试几个例子\n",
    "a = torch.tensor([2.0])\n",
    "b = torch.tensor([3.0])\n",
    "c = torch.mul(a, b)\n",
    "\n",
    "print(c)\n",
    "print(a.is_leaf)\n",
    "print(b.is_leaf)\n",
    "print(c.is_leaf)\n",
    "\n",
    "# 我才tm发现 有一个大写的Tensor 还有一个小写的tensor\n",
    "# cao 小写的就可以指定为grad了\n",
    "# tensor这个函数可以返回一个大写的Tensor\n",
    "a = torch.tensor(2.0, requires_grad=True)\n",
    "b = torch.tensor(3.0)\n",
    "c = torch.mul(a, b)\n",
    "\n",
    "print(c)\n",
    "print(a.is_leaf)\n",
    "print(a.requires_grad)\n",
    "print(b.is_leaf)\n",
    "print(b.requires_grad)\n",
    "print(c.is_leaf)\n",
    "print(c.requires_grad)\n",
    "\n",
    "c.backward()\n",
    "# c = a * b\n",
    "# 6 = 2 * 3\n",
    "# a.grad = dc_da = b = 3\n",
    "print(a.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251134.53989979601\n",
      "184425.07524261376\n",
      "137134.28863682644\n",
      "103904.85995904237\n",
      "80541.54141590775\n",
      "64023.0742580964\n",
      "52229.31058927747\n",
      "43696.93973841839\n",
      "37425.95398606721\n",
      "32734.165354433553\n",
      "29154.379977684563\n",
      "26365.334110161777\n",
      "24145.372534754177\n",
      "22341.405333758314\n",
      "20847.68215371242\n",
      "19590.6415633661\n",
      "18518.201680442464\n",
      "17592.5446745026\n",
      "16785.351320797672\n",
      "16074.742839617378\n",
      "15443.358714986385\n",
      "14877.160650588869\n",
      "14364.713586253913\n",
      "13896.766587192189\n",
      "13465.982095774587\n",
      "13066.695000042579\n",
      "12694.59137292023\n",
      "12346.363255098318\n",
      "12019.401481423263\n",
      "11711.545013022262\n",
      "11420.912593822603\n",
      "11145.84802122177\n",
      "10884.942572241329\n",
      "10637.073192262662\n",
      "10401.434959344891\n",
      "10177.507855925975\n",
      "9964.908713010294\n",
      "9763.169501729983\n",
      "9571.610728438212\n",
      "9389.389145795732\n",
      "9215.657604301408\n",
      "9049.707328029828\n",
      "8891.034126764198\n",
      "8739.328814106551\n",
      "8594.42438347288\n",
      "8456.207941366369\n",
      "8324.51647918453\n",
      "8199.087617896508\n",
      "8079.606526649339\n",
      "7965.7612341453\n",
      "7857.260768742316\n",
      "7753.81898609514\n",
      "7655.115438488209\n",
      "7560.821268701158\n",
      "7470.660485780874\n",
      "7384.422469918167\n",
      "7301.955997276528\n",
      "7223.124547719714\n",
      "7147.7741145686905\n",
      "7075.7292263358\n",
      "7006.796599793979\n",
      "6940.784433768233\n",
      "6877.508440703556\n",
      "6816.803837030684\n",
      "6758.514432950265\n",
      "6702.489742558348\n",
      "6648.591519463502\n",
      "6596.697398072178\n",
      "6546.699488324079\n",
      "6498.498302532738\n",
      "6452.013416508953\n",
      "6407.190942291904\n",
      "6363.9947822145205\n",
      "6322.382208768481\n",
      "6282.290736832616\n",
      "6243.632795784161\n",
      "6206.309535662024\n",
      "6170.225675058555\n",
      "6135.299080379525\n",
      "6101.475766063533\n",
      "6068.727013959407\n",
      "6037.026207698282\n",
      "6006.3375829855495\n",
      "5976.626565032182\n",
      "5947.849845845404\n",
      "5919.967967363333\n",
      "5892.950142216053\n",
      "5866.755194886742\n",
      "5841.354651470776\n",
      "5816.721195034694\n",
      "5792.831394092558\n",
      "5769.665918759101\n",
      "5747.198050841464\n",
      "5725.404431564052\n",
      "5704.259670297022\n",
      "5683.74443772603\n",
      "5663.836701762745\n",
      "5644.509653323287\n",
      "5625.749734743427\n",
      "5607.54248076703\n",
      "5589.869337626366\n",
      "5572.714036448013\n",
      "5556.059372194133\n",
      "5539.886160692625\n",
      "5524.175926785829\n",
      "5508.917481956786\n",
      "5494.100539667543\n",
      "5479.71342306471\n",
      "5465.745733924818\n",
      "5452.184258039365\n",
      "5439.016128747775\n",
      "5426.232926661487\n",
      "5413.8251531403275\n",
      "5401.782255186461\n",
      "5390.09257284653\n",
      "5378.7441680513275\n",
      "5367.724456334821\n",
      "5357.020794656997\n",
      "5346.6224493876925\n",
      "5336.518696544748\n",
      "5326.698513088824\n",
      "5317.1521053455435\n",
      "5307.870625513233\n",
      "5298.845485847922\n",
      "5290.067544024811\n",
      "5281.528487489242\n",
      "5273.219654290347\n",
      "5265.13243807984\n",
      "5257.2579861449485\n",
      "5249.587834948148\n",
      "5242.114701180559\n",
      "5234.8329699267715\n",
      "5227.7349189188\n",
      "5220.812703868731\n",
      "5214.061263842621\n",
      "5207.475221045123\n",
      "5201.0477775280015\n",
      "5194.7742962365655\n",
      "5188.649781758362\n",
      "5182.668218181067\n",
      "5176.825592427296\n",
      "5171.117164187613\n",
      "5165.538064210643\n",
      "5160.084760392259\n",
      "5154.752844598612\n",
      "5149.5383345877635\n",
      "5144.437868414372\n",
      "5139.447372069154\n",
      "5134.563327782088\n",
      "5129.782262447681\n",
      "5125.100553785672\n",
      "5120.517356289689\n",
      "5116.0296507359035\n",
      "5111.632314185552\n",
      "5107.318473868898\n",
      "5103.092910634448\n",
      "5098.953846111271\n",
      "5094.888291484178\n",
      "5090.9076835579945\n",
      "5087.0048712374555\n",
      "5083.167822780717\n",
      "5079.403407922631\n",
      "5075.712061348271\n",
      "5072.092264257273\n",
      "5068.529195450579\n",
      "5065.030004581538\n",
      "5061.60237006739\n",
      "5058.231509356114\n",
      "5054.919596918987\n",
      "5051.6684686037515\n",
      "5048.473377010264\n",
      "5045.332284054345\n",
      "5042.248392913453\n",
      "5039.21644103891\n",
      "5036.23300001165\n",
      "5033.304813546982\n",
      "5030.426944372362\n",
      "5027.595853544549\n",
      "5024.811770514136\n",
      "5022.074986119964\n",
      "5019.387429588705\n",
      "5016.743246315658\n",
      "5014.141810406525\n",
      "5011.587486338054\n",
      "5009.076155769947\n",
      "5006.604961663852\n",
      "5004.1763784259\n",
      "5001.786082509745\n",
      "4999.433422357418\n",
      "4997.122995974651\n",
      "4994.850296064566\n",
      "4992.612580286477\n",
      "4990.413651926964\n",
      "4988.250564308852\n",
      "4986.122518402737\n",
      "4984.029652467341\n",
      "4981.969144706897\n",
      "4979.941388996473\n",
      "4977.945255444612\n",
      "4975.979939794126\n"
     ]
    }
   ],
   "source": [
    "# 现在正式开始写\n",
    "\n",
    "def train_loop():\n",
    "    # load data\n",
    "    X, W, b, num_movies, num_features, num_users = load_precalc_params_small()\n",
    "    # Y, R = load_ratings_small()\n",
    "\n",
    "    # 好像这个数据并不是我们需要的格式\n",
    "    # 我们也并没有对数据进行nomalization\n",
    "    # Reload ratings and add new ratings\n",
    "    Y, R = load_ratings_small()\n",
    "\n",
    "    # Normalize the Dataset\n",
    "    Ynorm, Ymean = normalizeRatings(Y, R)\n",
    "\n",
    "    # 要不随机一下初始参数\n",
    "    # X_tensor = torch.tensor(X, requires_grad=True)\n",
    "    R_tensor = torch.tensor(R)\n",
    "    Y_tensor = torch.tensor(Ynorm)\n",
    "    # 是不是复制别人的参数就不能作为grad\n",
    "    # 但是可以手动设置requires_grad=True\n",
    "    # W_tensor = torch.tensor(W, requires_grad=True)\n",
    "    # b_tensor = torch.tensor(b, requires_grad=True)\n",
    "\n",
    "    X_tensor = torch.randn(num_movies, num_features, requires_grad=True)\n",
    "    W_tensor = torch.randn(num_users, num_features, requires_grad=True)\n",
    "    b_tensor = torch.randn(1, num_users, requires_grad=True)\n",
    "\n",
    "    lambda_ = 1\n",
    "\n",
    "    # 为什么换个算法就行了??\n",
    "    # 从SGD换成Adam就可以了??\n",
    "    optimizer = torch.optim.Adam([W_tensor, b_tensor, X_tensor], lr=0.1)\n",
    "    epoch = 200\n",
    "    # 为什么pytorch和tensorflow用相同的参数相同的实现 但是pytorch收敛的这么慢\n",
    "    for e in range(epoch):\n",
    "\n",
    "        # prediction\n",
    "\n",
    "        # forward\n",
    "        # 为什么loss越来越大\n",
    "        J = torch.sum(torch.mul(R_tensor, torch.pow(torch.mm(X_tensor, W_tensor.t()) + b_tensor - Y_tensor, 2))) / 2\n",
    "        # considier regularization\n",
    "        J += lambda_ * torch.sum(torch.pow(W_tensor, 2)) / 2\n",
    "        J += lambda_ * torch.sum(torch.pow(X_tensor, 2)) / 2\n",
    "        # backward\n",
    "        print(J.item())\n",
    "        J.backward()\n",
    "        # update\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "\n",
    "train_loop()\n",
    "\n",
    "# 我还是先简单的拟合一个线性模型吧\n",
    "# 为什么 线性模型就没有问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 然后是进行推荐\n",
    "# 对所有用户都可以进行推荐\n",
    "\n",
    "# 所谓推荐其实就是根据用户的特征向量和电影的特征向量进行相乘\n",
    "# 其实就是一个prediction\n",
    "p = torch.mm(X_tensor, W_tensor.t()) + b_tensor\n",
    "# 然后加上用户的平均评分\n",
    "p += Ymean\n",
    "\n",
    "# 每一列代表一个用户\n",
    "# 我们最好可以去掉用户已经评分的电影\n",
    "# 然后再进行排序\n",
    "# 可以只显示前十\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
